{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Challenge\n",
    "\n",
    "Este notebook documenta la solución completa al desafío, incluyendo la organización del repositorio, los enfoques de optimización en tiempo y memoria, el benchmarking de los backends y un anexo de verificación de resultados.\n",
    "\n",
    "---\n",
    "\n",
    "En primer lugar, se debe descargar el archivo `farmers-protest-tweets-2021-2-4.json` y almacenarlo en una carpeta llamada `data` en la raiz. Esto puede hacerse manualmente o ejecutando el siguiente bloque de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo JSON ya existe en: ../data/farmers-protest-tweets-2021-2-4.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "json_path = data_dir / \"farmers-protest-tweets-2021-2-4.json\"\n",
    "zip_path = data_dir / \"farmers-protest-tweets-2021-2-4.json.zip\"\n",
    "\n",
    "if not json_path.exists():\n",
    "    if not zip_path.exists():\n",
    "        try:\n",
    "            import gdown\n",
    "        except ImportError:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\"], check=True)\n",
    "            import gdown\n",
    "\n",
    "        url = \"https://drive.google.com/uc?id=1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis\"\n",
    "        print(f\"Descargando ZIP a {zip_path} ...\")\n",
    "        gdown.download(url, str(zip_path), quiet=False)\n",
    "\n",
    "    print(f\"Descomprimiendo {zip_path} ...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(path=data_dir)\n",
    "\n",
    "    zip_path.unlink()\n",
    "\n",
    "    print(f\"Archivo JSON disponible en: {json_path}\")\n",
    "else:\n",
    "    print(f\"El archivo JSON ya existe en: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estructura del repositorio\n",
    "\n",
    "La estructura principal del repositorio es la siguiente:\n",
    "\n",
    "```\n",
    "| .\n",
    "| ├── data/\n",
    "| │   └── farmers-protest-tweets-2021-2-4.json   <-- Archivo del desafío\n",
    "| ├── src/\n",
    "| |   ├── challenge.ipynb                  <-- Notebook principal con la solución al desafío\n",
    "| │   ├── backend/\n",
    "| │   │   ├── pandas_backend.py            <-- Funciones utilizando Pandas\n",
    "| │   │   ├── polars_backend.py            <-- Funciones utilizando Polars\n",
    "| │   │   ├── duckdb_backend.py            <-- Funciones utilizando DuckDB\n",
    "| │   │   └── ijson_backend.py             <-- Funciones utilizando ijson\n",
    "| │   ├── q1_time.py                       <-- q1 optimizada en tiempo\n",
    "| │   ├── q1_memory.py                     <-- q1 optimizada en memoria\n",
    "| │   ├── q2_time.py                       <-- q2 optimizada en tiempo\n",
    "| │   ├── q2_memory.py                     <-- q2 optimizada en memoria\n",
    "| │   ├── q3_time.py                       <-- q3 optimizada en tiempo\n",
    "| │   ├── q3_memory.py                     <-- q3 optimizada en memoria\n",
    "| │   └── benchmark/\n",
    "| │       ├── benchmark_script.py          <-- Script independiente de benchmarking\n",
    "| │       └── benchmark.ipynb              <-- Notebook de benchmarking detallado\n",
    "| ├── profile_reports/\n",
    "| │   ├── cprofile_combined.txt            <-- Reporte único de cProfile\n",
    "| │   └── master_summary.csv               <-- CSV con todas las métricas\n",
    "| ├── run_all_benchmarks.sh                <-- Script maestro para invocar el benchmarking\n",
    "| ├── requirements.txt                     <-- Versiones de librerías usadas\n",
    "| ├── tests/\n",
    "| │   ├── conftest.py                      <-- Fixture de pytest que parametriza backends\n",
    "| │   ├── data_fixtures.py                 <-- Creación de archivos NDJSON temporales\n",
    "| │   ├── loaders/\n",
    "| │   │   ├── test_duckdb_loader.py        <-- Tests de helpers de DuckDB\n",
    "| │   │   ├── test_pandas_loader.py        <-- Tests de helpers de Pandas\n",
    "| │   │   └── test_polars_loader.py        <-- Tests de helpers de Polars\n",
    "| │   └── business/\n",
    "| │       ├── test_top_active_dates.py     <-- Tests de negocio para top_active_dates\n",
    "| │       ├── test_top_emojis.py           <-- Tests de negocio para top_emojis\n",
    "| │       └── test_top_mentioned_users.py  <-- Tests de negocio para top_mentioned_users\n",
    "| └── README.md                            <-- Descripción general y pasos para ejecutar\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo\n",
    "data_path = Path(\"../data/farmers-protest-tweets-2021-2-4.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enfoque general\n",
    "\n",
    "1. **Separación de funciones y backends**  \n",
    "   Cada pregunta (q1, q2 y q3) se resuelve con dos funciones:\n",
    "   - Una versión **optimizada en tiempo** (`qX_time.py`), usando polars.\n",
    "   - Una versión **optimizada en memoria** (`qX_memory.py`), usando `ijson`.\n",
    "\n",
    "2. **Backends modulares en `src/backend/`**  \n",
    "   - `pandas_backend.py`  \n",
    "   - `polars_backend.py`  \n",
    "   - `duckdb_backend.py`  \n",
    "   - `ijson_backend.py`  \n",
    "   Cada uno expone funciones genéricas (`top_active_dates`, `top_emojis`, `top_mentioned_users`) que se emplean para medir rendimiento.\n",
    "\n",
    "3. **Benchmark independiente**  \n",
    "   - El archivo `src/benchmark/benchmark_script.py` ejecuta cada combinación `backend + función` en un **proceso aislado**.  \n",
    "   - El script genera un CSV acumulativo (`profile_reports/master_summary.csv`) y opcionalmente un único archivo de texto con todos los reportes de `cProfile`.  \n",
    "   - El script maestro `run_all_benchmarks.sh` lanza las 12 pruebas (4 backends × 3 funciones).\n",
    "\n",
    "4. **Notebook de benchmark**  \n",
    "   Para más detalles del benchmark, detalles del optimo de memoria/tiempo y como se escogión la opción óptima para cada caso, ir al notebook correspondiente al benchmarking.\n",
    "   - `src/benchmark/benchmark.ipynb` importa `profile_reports/master_summary.csv` y genera gráficos comparativos por función (tiempo y memoria).  \n",
    "  A continuación podemos ver un pequeño cuadro resumen del benchmark:\n",
    "\n",
    "| Función               | Backend | Tiempo (s) | Peak de memoria (MB) |\n",
    "|-----------------------|---------|------------|-------------------|\n",
    "| **top_active_dates**  | pandas  | 1.977      | 515.30            |\n",
    "|                       | polars  | 0.753      | 751.41            |\n",
    "|                       | duckdb  | 0.898      | 1626.23           |\n",
    "|                       | ijson   | 2.749      | 101.88            |\n",
    "| **top_emojis**        | pandas  | 1.716      | 187.78            |\n",
    "|                       | polars  | 0.109      | 549.78            |\n",
    "|                       | duckdb  | 0.646      | 534.25            |\n",
    "|                       | ijson   | 1.853      | 90.03             |\n",
    "| **top_mentioned_users** | pandas  | 1.741      | 274.47            |\n",
    "|                         | polars  | 0.354      | 1241.73           |\n",
    "|                         | duckdb  | 0.403      | 847.69            |\n",
    "|                         | ijson   | 2.920      | 94.48             |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Librerías y herramientas usadas\n",
    "\n",
    "- **cProfile** + **pstats**: para medición detallada de tiempo de CPU, número de llamadas y estadísticas acumuladas.  \n",
    "- **memory-profiler**: para muestreo y captura del peak de memoria residente durante la ejecución.  \n",
    "- **ijson**: para lecturas en streaming y minimización de uso de memoria en las funciones optimizadas en memoria.  \n",
    "- **Pandas**, **Polars**, **DuckDB**: diferentes enfoques en tiempo con estructuras tabulares/columnar/SQL embebido.  \n",
    "- **Matplotlib**: para graficar comparaciones de tiempo y memoria en el notebook de benchmarking.  \n",
    "- **Argparse**: en `benchmark_script.py` para recibir parámetros de línea de comandos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solución detallada de cada pregunta\n",
    "\n",
    "### 4.1. Q1: Top 10 fechas con más tweets (+ usuario que más tuitea)\n",
    "\n",
    "- **Versión optimizada en tiempo**  \n",
    "  Se usó **Polars** para:\n",
    "  1. `scan_ndjson()` para leer únicamente las columnas `date` y `user`.\n",
    "  2. Convertir el campo de fecha a tipo `Datetime` y luego extraer la columna `dt` (solo fecha).\n",
    "  3. Extraer `user.username` en forma de columna.\n",
    "  4. Agrupar por `(dt, username)` y contar en paralelo.\n",
    "  5. Obtener el usuario con mayor count por fecha (`group_by(\"dt\").first()` tras ordenar).\n",
    "  6. Calcular el total de tweets por `dt` y ordenar para elegir las 10 fechas con más tweets.\n",
    "\n",
    "- **Versión optimizada en memoria**  \n",
    "  Se usó **ijson** para:\n",
    "  1. Abrir el archivo en modo streaming y recorrer cada tweet uno a uno.\n",
    "  2. Para cada tweet, extraer la fecha (`fecha = date.fromisoformat(date_str[:10])`) y `username`.\n",
    "  3. Incrementar un `Counter` global para contar tweets totales por fecha.\n",
    "  4. Mantener un diccionario de `Counter` por fecha que acumula conteo de tuits por usuario.\n",
    "  5. Al finalizar la lectura, calcular manualmente las 10 fechas con mayor total y extraer el usuario más frecuente en cada fecha.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1 optimizada en memoria\n",
    "from q1_memory import q1_memory\n",
    "\n",
    "q1_memory_result = q1_memory(data_path)\n",
    "q1_memory_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1 optimizada en tiempo\n",
    "from q1_time import q1_time\n",
    "\n",
    "q1_time_result = q1_time(data_path)\n",
    "q1_time_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Q2: Top 10 emojis más usados (con su conteo)\n",
    "\n",
    "- **Versión optimizada en tiempo**  \n",
    "  - Se usó **Polars** para:  \n",
    "    1. `scan_ndjson()` lee únicamente la columna `content` en modo lazy.  \n",
    "    2. Filtra tweets con contenido no nulo.  \n",
    "    3. Crea una expresión que extrae todos los emojis usando una expresión regular Unicode (`\\p{Extended_Pictographic}`) en forma vectorizada.  \n",
    "    4. Aplana la lista de emojis, agrupa por cada símbolo y cuenta en paralelo.  \n",
    "    5. Ordena por frecuencia y toma los 10 emojis más frecuentes.  \n",
    "\n",
    "- **Versión optimizada en memoria**  \n",
    "  - Se usó **ijson** para:  \n",
    "    1. Abrir el archivo NDJSON en modo streaming y procesar tweet a tweet.  \n",
    "    2. Para cada tweet, extraer el campo `content` y aplicar un patrón `regex` Python `r\"\\p{Extended_Pictographic}\"` para extraer emojis.  \n",
    "    3. Mantener un `Counter` que acumula la frecuencia de cada emoji encontrado.  \n",
    "    4. Al terminar, devolver los 10 emojis más comunes.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 7286),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('✊', 2411),\n",
       " ('🌾', 2363),\n",
       " ('❤', 1779),\n",
       " ('🤣', 1668),\n",
       " ('👇', 1108),\n",
       " ('💚', 1040),\n",
       " ('💪', 947)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2 optimizada en memoria\n",
    "from q2_memory import q2_memory\n",
    "\n",
    "q2_memory_result = q2_memory(data_path)\n",
    "q2_memory_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 7286),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('✊', 2411),\n",
       " ('🌾', 2363),\n",
       " ('❤', 1779),\n",
       " ('🤣', 1668),\n",
       " ('👇', 1108),\n",
       " ('💚', 1040),\n",
       " ('💪', 947)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2 optimizada en tiempo\n",
    "from q2_time import q2_time\n",
    "\n",
    "q2_time_result = q2_time(data_path)\n",
    "q2_time_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Q3: Top 10 usuarios más mencionados (conteo de `@username`)\n",
    "\n",
    "- **Versión optimizada en tiempo**  \n",
    "  - Se usó **Polars** para:  \n",
    "    1. `scan_ndjson()` lee únicamente la columna `mentionedUsers` en modo lazy.  \n",
    "    2. Explota la columna de listas de menciones (`explode(\"mentionedUsers\")`).  \n",
    "    3. Mapea cada elemento (un objeto JSON) a su campo `username`.  \n",
    "    4. Filtra nulos, agrupa por cada `username` y cuenta en paralelo.  \n",
    "    5. Ordena por frecuencia y toma los 10 usuarios más mencionados.  \n",
    "\n",
    "- **Versión optimizada en memoria**  \n",
    "  - Se usa **ijson** para:  \n",
    "    1. Leer el NDJSON en streaming, tweet a tweet.  \n",
    "    2. Para cada tweet, extraer la lista `mentionedUsers`.  \n",
    "    3. Iterar esa lista y, por cada diccionario de mención, extraer `username` y actualizar un `Counter`.  \n",
    "    4. Al finalizar la lectura, retornar los 10 usuarios más frecuentes del `Counter`.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3 optimizada en memoria\n",
    "from q3_memory import q3_memory\n",
    "\n",
    "q3_memory_result = q3_memory(data_path)\n",
    "q3_memory_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3 optimizada en tiempo\n",
    "from q3_time import q3_time\n",
    "\n",
    "q3_time_result = q3_time(data_path)\n",
    "q3_time_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Unit Testing\n",
    "\n",
    "Para garantizar la correcta funcionalidad —y la consistencia entre backends— se implementó una batería completa de tests con **pytest**, organizada en `tests/`.\n",
    "\n",
    "### 5.1. Estrategia\n",
    "\n",
    "1. **Parametrización por backend**  \n",
    "   - Con `conftest.py` se importa dinámicamente cada módulo (`pandas_backend`, `polars_backend`, `duckdb_backend`, `ijson_backend`).  \n",
    "   - Cada test de negocio (`top_active_dates`, `top_emojis`, `top_mentioned_users`) se ejecuta **cuatro veces**, una por backend.\n",
    "\n",
    "2. **Fixtures auto-contenidas**  \n",
    "   - `tests/data_fixtures.py` crea archivos NDJSON temporales usando `tmp_path`, de modo que los tests no dependen de archivos reales ni de conexión externa.  \n",
    "   - Se cubren escenarios:\n",
    "     - Datos “felices” (`sample_data_path`)  \n",
    "     - Casos con valores nulos (`sample_data_with_nulls`)  \n",
    "     - Dataset sin emojis (`sample_data_no_emoji`)\n",
    "\n",
    "3. **Cobertura de helpers**  \n",
    "   - Se testean también las funciones auxiliares internas (`_lazy_scan`, `_load_ndjson`, `_get_connection`, etc.), verificando manejo de rutas inexistentes y retorno de estructuras correctas.\n",
    "\n",
    "### 5.2. Resultados de cobertura\n",
    "\n",
    "| Name                            | Stmts | Miss | Cover |\n",
    "|---------------------------------|-------|------|-------|\n",
    "| src/backend/__init__.py         | 0     | 0    | 100%  |\n",
    "| src/backend/duckdb_backend.py   | 59    | 3    | 95%   |\n",
    "| src/backend/ijson_backend.py    | 92    | 14   | 85%   |\n",
    "| src/backend/pandas_backend.py   | 56    | 3    | 95%   |\n",
    "| src/backend/polars_backend.py   | 39    | 3    | 92%   |\n",
    "| **TOTAL**                       | 246   | 23   | 91%   |\n",
    "\n",
    "- **Cobertura global:** **91 %** (44 tests, ~0.6 s).  \n",
    "- **Rango por archivo:** 85 % – 95 %.\n",
    "\n",
    "### 5.3. Deuda técnica\n",
    "\n",
    "Aunque un 91 % es aceptable, mi objetivo habitual es más de 95 %.  \n",
    "Por limitaciones de tiempo dejé pendiente:\n",
    "\n",
    "| Archivo | Pendiente | Impacto |\n",
    "|---------|-----------|---------|\n",
    "| `ijson_backend.py` | Verificar rutas de error en `_parse_ndjson_stream` y branches que manejan objetos JSON anidados poco comunes. | +3/4 % cobertura |\n",
    "| `duckdb_backend.py` | Tests sobre manejo de tweets sin campo `mentionedUsers` y fechas malformadas. | +1/2 % cobertura |\n",
    "| Casos borde globales | Archivos vacíos, líneas en blanco múltiples, mezclas de codificaciones. | Robustece producción |\n",
    "\n",
    "Esta mejora queda como **deuda técnica**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones finales y elección de frameworks\n",
    "\n",
    "1. **Resultados clave**  \n",
    "   - **Polars** fue el backend más rápido en las tres funciones.  \n",
    "   - **ijson** alcanzó siempre el menor peak de memoria.  \n",
    "   - **Pandas** ofreció un equilibrio razonable (tiempo medio, memoria contenida).  \n",
    "   - **DuckDB** igualó a Polars en tiempo en algunos casos, pero con mayor uso de RAM.\n",
    "\n",
    "2. **Supuesto: ejecución local**  \n",
    "   Aunque el desafío permite usar plataformas cloud, decidí perfilar en modo single-node por:\n",
    "   - **Volumen**: 400 MB caben cómodamente en la RAM de un portátil; no se requiere cluster.  \n",
    "   - **Reproducibilidad rápida**: quien revise el repo puede correr todo en su máquina sin credenciales externas.  \n",
    "   - **Plazo corto** (4 días): evita sobrecarga de aprovisionar servicios.\n",
    "\n",
    "   > *Este es un supuesto mío; no es una exigencia del enunciado.*\n",
    "\n",
    "3. **Cuándo migrar a BigQuery / Dataflow / DataProc**  \n",
    "   | Escenario | Ventaja cloud-scale |\n",
    "   |-----------|--------------------|\n",
    "   | Datasets de **10 GB – TB** | Almacén columnar distribuido (BigQuery) o Spark (DataProc) para escaneo paralelo. |\n",
    "   | **ETL recurrente** | Apache Beam (runner Dataflow) gestiona ventanas, reintentos y autoscaling. |\n",
    "   | **Consultas ad-hoc multi-usuario** | BigQuery ofrece capacidad elástica sin afectar a otros workloads. |\n",
    "   | **SLA estrictos** o alta concurrencia | Delegar tolerancia a fallos y escalado horizontal al proveedor. |\n",
    "\n",
    "4. **Conclusión práctica**  \n",
    "   - Para el dataset del reto, **Polars + ijson** cubren los extremos de velocidad y frugalidad en RAM con complejidad mínima.  \n",
    "   - En proyectos productivos con fuentes continuas o volúmenes muy superiores, escalar a servicios cloud sería lo apropiado; las funciones aquí implementadas podrían portarse a PySpark, Beam o consultas SQL en BigQuery sin cambiar la lógica de negocio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Anexo - Ejecución de todos los backends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Top 10 dates with the most tweets and their most active user:**\n",
      "\n",
      " 1. Date: 2021-02-12   —   User with the most tweets: RanbirS00614606\n",
      " 2. Date: 2021-02-13   —   User with the most tweets: MaanDee08215437\n",
      " 3. Date: 2021-02-17   —   User with the most tweets: RaaJVinderkaur\n",
      " 4. Date: 2021-02-16   —   User with the most tweets: jot__b\n",
      " 5. Date: 2021-02-14   —   User with the most tweets: rebelpacifist\n",
      " 6. Date: 2021-02-18   —   User with the most tweets: neetuanjle_nitu\n",
      " 7. Date: 2021-02-15   —   User with the most tweets: jot__b\n",
      " 8. Date: 2021-02-20   —   User with the most tweets: MangalJ23056160\n",
      " 9. Date: 2021-02-23   —   User with the most tweets: Surrypuria\n",
      "10. Date: 2021-02-19   —   User with the most tweets: Preetm91\n",
      "\n",
      "**Top 10 most used emojis (emoji — frequency):**\n",
      "\n",
      " 1. 🙏   —   7286\n",
      " 2. 😂   —   3072\n",
      " 3. 🚜   —   2972\n",
      " 4. ✊   —   2411\n",
      " 5. 🌾   —   2363\n",
      " 6. ❤   —   1779\n",
      " 7. 🤣   —   1668\n",
      " 8. 👇   —   1108\n",
      " 9. 💚   —   1040\n",
      "10. 💪   —   947\n",
      "\n",
      "**Top 10 most mentioned users (user — frequency):**\n",
      "\n",
      " 1. narendramodi   —   2265\n",
      " 2. Kisanektamorcha   —   1840\n",
      " 3. RakeshTikaitBKU   —   1644\n",
      " 4. PMOIndia   —   1427\n",
      " 5. RahulGandhi   —   1146\n",
      " 6. GretaThunberg   —   1048\n",
      " 7. RaviSinghKA   —   1019\n",
      " 8. rihanna   —   986\n",
      " 9. UNHumanRights   —   962\n",
      "10. meenaharris   —   926\n"
     ]
    }
   ],
   "source": [
    "from backend.polars_backend import (\n",
    "    top_active_dates,\n",
    "    top_emojis,\n",
    "    top_mentioned_users\n",
    ")\n",
    "\n",
    "# Top Active Dates\n",
    "n_dates = 10\n",
    "\n",
    "active_dates = top_active_dates(data_path, n=n_dates)\n",
    "\n",
    "print(f\"**Top {n_dates} dates with the most tweets and their most active user:**\\n\")\n",
    "for idx, (date, user) in enumerate(active_dates, start=1):\n",
    "    print(f\"{idx:2d}. Date: {date}   —   User with the most tweets: {user}\")\n",
    "\n",
    "# Top Emojis\n",
    "n_emojis = 10\n",
    "\n",
    "most_used_emojis = top_emojis(data_path, n=n_emojis)\n",
    "\n",
    "print(f\"\\n**Top {n_emojis} most used emojis (emoji — frequency):**\\n\")\n",
    "for idx, (emoji, frequency) in enumerate(most_used_emojis, start=1):\n",
    "    print(f\"{idx:2d}. {emoji}   —   {frequency}\")\n",
    "\n",
    "# Top Mentions\n",
    "n_mentions = 10\n",
    "\n",
    "most_mentioned_users = top_mentioned_users(data_path, n=n_mentions)\n",
    "\n",
    "print(f\"\\n**Top {n_mentions} most mentioned users (user — frequency):**\\n\")\n",
    "for idx, (user, count) in enumerate(most_mentioned_users, start=1):\n",
    "    print(f\"{idx:2d}. {user}   —   {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Top 10 dates with the most tweets and their most active user:**\n",
      "\n",
      " 1. Date: 2021-02-12   —   User with the most tweets: RanbirS00614606\n",
      " 2. Date: 2021-02-13   —   User with the most tweets: MaanDee08215437\n",
      " 3. Date: 2021-02-17   —   User with the most tweets: RaaJVinderkaur\n",
      " 4. Date: 2021-02-16   —   User with the most tweets: jot__b\n",
      " 5. Date: 2021-02-14   —   User with the most tweets: rebelpacifist\n",
      " 6. Date: 2021-02-18   —   User with the most tweets: neetuanjle_nitu\n",
      " 7. Date: 2021-02-15   —   User with the most tweets: jot__b\n",
      " 8. Date: 2021-02-20   —   User with the most tweets: MangalJ23056160\n",
      " 9. Date: 2021-02-23   —   User with the most tweets: Surrypuria\n",
      "10. Date: 2021-02-19   —   User with the most tweets: Preetm91\n",
      "\n",
      "**Top 10 most used emojis (emoji — frequency):**\n",
      "\n",
      " 1. 🙏   —   7286\n",
      " 2. 😂   —   3072\n",
      " 3. 🚜   —   2972\n",
      " 4. ✊   —   2411\n",
      " 5. 🌾   —   2363\n",
      " 6. ❤   —   1779\n",
      " 7. 🤣   —   1668\n",
      " 8. 👇   —   1108\n",
      " 9. 💚   —   1040\n",
      "10. 💪   —   947\n",
      "\n",
      "**Top 10 most mentioned users (user — frequency):**\n",
      "\n",
      " 1. narendramodi   —   2265\n",
      " 2. Kisanektamorcha   —   1840\n",
      " 3. RakeshTikaitBKU   —   1644\n",
      " 4. PMOIndia   —   1427\n",
      " 5. RahulGandhi   —   1146\n",
      " 6. GretaThunberg   —   1048\n",
      " 7. RaviSinghKA   —   1019\n",
      " 8. rihanna   —   986\n",
      " 9. UNHumanRights   —   962\n",
      "10. meenaharris   —   926\n"
     ]
    }
   ],
   "source": [
    "from backend.pandas_backend import (\n",
    "    top_active_dates,\n",
    "    top_emojis,\n",
    "    top_mentioned_users\n",
    ")\n",
    "\n",
    "# Top Active Dates\n",
    "n_dates = 10\n",
    "\n",
    "active_dates = top_active_dates(data_path, n=n_dates)\n",
    "\n",
    "print(f\"**Top {n_dates} dates with the most tweets and their most active user:**\\n\")\n",
    "for idx, (date, user) in enumerate(active_dates, start=1):\n",
    "    print(f\"{idx:2d}. Date: {date}   —   User with the most tweets: {user}\")\n",
    "\n",
    "# Top Emojis\n",
    "n_emojis = 10\n",
    "\n",
    "most_used_emojis = top_emojis(data_path, n=n_emojis)\n",
    "\n",
    "print(f\"\\n**Top {n_emojis} most used emojis (emoji — frequency):**\\n\")\n",
    "for idx, (emoji, frequency) in enumerate(most_used_emojis, start=1):\n",
    "    print(f\"{idx:2d}. {emoji}   —   {frequency}\")\n",
    "\n",
    "# Top Mentions\n",
    "n_mentions = 10\n",
    "\n",
    "most_mentioned_users = top_mentioned_users(data_path, n=n_mentions)\n",
    "\n",
    "print(f\"\\n**Top {n_mentions} most mentioned users (user — frequency):**\\n\")\n",
    "for idx, (user, count) in enumerate(most_mentioned_users, start=1):\n",
    "    print(f\"{idx:2d}. {user}   —   {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Top 10 dates with the most tweets and their most active user:**\n",
      "\n",
      " 1. Date: 2021-02-12   —   User with the most tweets: RanbirS00614606\n",
      " 2. Date: 2021-02-13   —   User with the most tweets: MaanDee08215437\n",
      " 3. Date: 2021-02-17   —   User with the most tweets: RaaJVinderkaur\n",
      " 4. Date: 2021-02-16   —   User with the most tweets: jot__b\n",
      " 5. Date: 2021-02-14   —   User with the most tweets: rebelpacifist\n",
      " 6. Date: 2021-02-18   —   User with the most tweets: neetuanjle_nitu\n",
      " 7. Date: 2021-02-15   —   User with the most tweets: jot__b\n",
      " 8. Date: 2021-02-20   —   User with the most tweets: MangalJ23056160\n",
      " 9. Date: 2021-02-23   —   User with the most tweets: Surrypuria\n",
      "10. Date: 2021-02-19   —   User with the most tweets: Preetm91\n",
      "\n",
      "**Top 10 most used emojis (emoji — frequency):**\n",
      "\n",
      " 1. 🙏   —   7286\n",
      " 2. 😂   —   3072\n",
      " 3. 🚜   —   2972\n",
      " 4. ✊   —   2411\n",
      " 5. 🌾   —   2363\n",
      " 6. ❤   —   1779\n",
      " 7. 🤣   —   1668\n",
      " 8. 👇   —   1108\n",
      " 9. 💚   —   1040\n",
      "10. 💪   —   947\n",
      "\n",
      "**Top 10 most mentioned users (user — frequency):**\n",
      "\n",
      " 1. narendramodi   —   2265\n",
      " 2. Kisanektamorcha   —   1840\n",
      " 3. RakeshTikaitBKU   —   1644\n",
      " 4. PMOIndia   —   1427\n",
      " 5. RahulGandhi   —   1146\n",
      " 6. GretaThunberg   —   1048\n",
      " 7. RaviSinghKA   —   1019\n",
      " 8. rihanna   —   986\n",
      " 9. UNHumanRights   —   962\n",
      "10. meenaharris   —   926\n"
     ]
    }
   ],
   "source": [
    "from backend.duckdb_backend import (\n",
    "    top_active_dates,\n",
    "    top_emojis,\n",
    "    top_mentioned_users\n",
    ")\n",
    "\n",
    "# Top Active Dates\n",
    "n_dates = 10\n",
    "\n",
    "active_dates = top_active_dates(data_path, n=n_dates)\n",
    "\n",
    "print(f\"**Top {n_dates} dates with the most tweets and their most active user:**\\n\")\n",
    "for idx, (date, user) in enumerate(active_dates, start=1):\n",
    "    print(f\"{idx:2d}. Date: {date}   —   User with the most tweets: {user}\")\n",
    "\n",
    "# Top Emojis\n",
    "n_emojis = 10\n",
    "\n",
    "most_used_emojis = top_emojis(data_path, n=n_emojis)\n",
    "\n",
    "print(f\"\\n**Top {n_emojis} most used emojis (emoji — frequency):**\\n\")\n",
    "for idx, (emoji, frequency) in enumerate(most_used_emojis, start=1):\n",
    "    print(f\"{idx:2d}. {emoji}   —   {frequency}\")\n",
    "\n",
    "# Top Mentions\n",
    "n_mentions = 10\n",
    "\n",
    "most_mentioned_users = top_mentioned_users(data_path, n=n_mentions)\n",
    "\n",
    "print(f\"\\n**Top {n_mentions} most mentioned users (user — frequency):**\\n\")\n",
    "for idx, (user, count) in enumerate(most_mentioned_users, start=1):\n",
    "    print(f\"{idx:2d}. {user}   —   {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Top 10 dates with the most tweets and their most active user:**\n",
      "\n",
      " 1. Date: 2021-02-12   —   User with the most tweets: RanbirS00614606\n",
      " 2. Date: 2021-02-13   —   User with the most tweets: MaanDee08215437\n",
      " 3. Date: 2021-02-17   —   User with the most tweets: RaaJVinderkaur\n",
      " 4. Date: 2021-02-16   —   User with the most tweets: jot__b\n",
      " 5. Date: 2021-02-14   —   User with the most tweets: rebelpacifist\n",
      " 6. Date: 2021-02-18   —   User with the most tweets: neetuanjle_nitu\n",
      " 7. Date: 2021-02-15   —   User with the most tweets: jot__b\n",
      " 8. Date: 2021-02-20   —   User with the most tweets: MangalJ23056160\n",
      " 9. Date: 2021-02-23   —   User with the most tweets: Surrypuria\n",
      "10. Date: 2021-02-19   —   User with the most tweets: Preetm91\n",
      "\n",
      "**Top 10 most used emojis (emoji — frequency):**\n",
      "\n",
      " 1. 🙏   —   7286\n",
      " 2. 😂   —   3072\n",
      " 3. 🚜   —   2972\n",
      " 4. ✊   —   2411\n",
      " 5. 🌾   —   2363\n",
      " 6. ❤   —   1779\n",
      " 7. 🤣   —   1668\n",
      " 8. 👇   —   1108\n",
      " 9. 💚   —   1040\n",
      "10. 💪   —   947\n",
      "\n",
      "**Top 10 most mentioned users (user — frequency):**\n",
      "\n",
      " 1. narendramodi   —   2265\n",
      " 2. Kisanektamorcha   —   1840\n",
      " 3. RakeshTikaitBKU   —   1644\n",
      " 4. PMOIndia   —   1427\n",
      " 5. RahulGandhi   —   1146\n",
      " 6. GretaThunberg   —   1048\n",
      " 7. RaviSinghKA   —   1019\n",
      " 8. rihanna   —   986\n",
      " 9. UNHumanRights   —   962\n",
      "10. meenaharris   —   926\n"
     ]
    }
   ],
   "source": [
    "from backend.ijson_backend import (\n",
    "    top_active_dates,\n",
    "    top_emojis,\n",
    "    top_mentioned_users\n",
    ")\n",
    "\n",
    "# Top Active Dates\n",
    "n_dates = 10\n",
    "\n",
    "active_dates = top_active_dates(data_path, n=n_dates)\n",
    "\n",
    "print(f\"**Top {n_dates} dates with the most tweets and their most active user:**\\n\")\n",
    "for idx, (date, user) in enumerate(active_dates, start=1):\n",
    "    print(f\"{idx:2d}. Date: {date}   —   User with the most tweets: {user}\")\n",
    "\n",
    "# Top Emojis\n",
    "n_emojis = 10\n",
    "\n",
    "most_used_emojis = top_emojis(data_path, n=n_emojis)\n",
    "\n",
    "print(f\"\\n**Top {n_emojis} most used emojis (emoji — frequency):**\\n\")\n",
    "for idx, (emoji, frequency) in enumerate(most_used_emojis, start=1):\n",
    "    print(f\"{idx:2d}. {emoji}   —   {frequency}\")\n",
    "\n",
    "# Top Mentions\n",
    "n_mentions = 10\n",
    "\n",
    "most_mentioned_users = top_mentioned_users(data_path, n=n_mentions)\n",
    "\n",
    "print(f\"\\n**Top {n_mentions} most mentioned users (user — frequency):**\\n\")\n",
    "for idx, (user, count) in enumerate(most_mentioned_users, start=1):\n",
    "    print(f\"{idx:2d}. {user}   —   {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
